{"nbformat": 4, "nbformat_minor": 1, "cells": [{"metadata": {}, "outputs": [], "source": ["\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics #import plot_roc_curve\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\ndf = pd.read_csv(\"UCI_Breast_cancer.csv\")\nprint(df.info())\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\n#Print the first 10 rows of the data\nprint(df.head(10))\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\n#Print the first 10 rows of the data\nprint(df.tail(10))\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<p>Do descriptive statistics on the data</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\ndf.describe()\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\n# Plot formatting\n\ndef plt_format():\n    %matplotlib inline\n    plt.rc('font', family='DejaVu Sans')\n    plt.figure(figsize=(16,14))\n    plt.rcParams['xtick.labelsize'] = 16\n    plt.rcParams['ytick.labelsize'] = 16\n    plt.rcParams['font.size'] = 16\n    sns.set(style=\"ticks\", color_codes=True)\n    plt.rcParams['axes.labelcolor'] = 'black'\n    plt.rcParams['axes.labelsize'] = 16\n    plt.rcParams['axes.labelweight'] = 'bold'\n    plt.rcParams['axes.titlesize'] = 32\n    plt.rcParams['axes.titleweight'] = 'bold'\n    plt.rcParams['text.color'] = 'black'\n    plt.rcParams['xtick.labelsize'] = 16\n    plt.rcParams['ytick.labelsize'] = 16\n    plt.rcParams['legend.frameon'] = False\n    plt.rcParams['axes.linewidth'] = 1\n    \nplt_format()\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\n# Check correlation among the variables.\n# Pearson correlation\n\nplt_format()\nax = plt.axes()\nsns.heatmap(df[df.columns[0:10]].corr(), annot=True)\nax.set_title('Heatmap of Pearson Correlation\\n')\nplt.axis('tight')\nplt.show()\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\n# Set the target variable which in this case \"Classification\" variable\n# or y = df.Classification.values\n# or y = dataset.iloc[:,len(dataset.iloc[0])-1].values\ny = df['Classification']  # Labels\n\n# Take the values of all rows for the first 9 variables as input \nX = df.iloc[:,0:9].values\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\n#Split dataset into training set and test set (e.g., 70% training, 30% test data)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\ndf.columns\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\n# Get a series containing counts of unique values of variable 'Classification'\n# Use normalize=true to get the relative frequencies of the unique values\ndf['Classification'].value_counts(normalize=True)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<p>This means that the data consists of 55% of class 2 and 45% of class 1.</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\n# Create RandomForestClassifier\nrf_classifier = RandomForestClassifier(n_estimators = 100, random_state=0)\n\n# Could use the best model from the cross-validation experiment below.\n# rf_classifier = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n#            max_depth=None, max_features='auto', max_leaf_nodes=None,\n#            min_impurity_decrease=0.0, min_impurity_split=None,\n#            min_samples_leaf=1, min_samples_split=2,\n#            min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=None,\n#            oob_score=True, random_state=0, verbose=0, warm_start=False)\n\n# Fit/train a model using training set\nrf_classifier.fit(X_train, y_train)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\n# Score it on your testing data.\n# rf_classifier.score(X_test, y_test)\n\n# Or nicer:\nprint(\"Accuracy on the Test data:\", rf_classifier.score(X_test, y_test))\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\ny_pred = rf_classifier.predict(X_test)\nprint(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\nimportances = rf_classifier.feature_importances_\n\nstd = np.std([tree.feature_importances_ for tree in rf_classifier.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(X_train.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(X_train.shape[1]), importances[indices],\n       color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(X_train.shape[1]), indices)\nplt.xlim([-1, X_train.shape[1]])\nplt.show()\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<p>Let's experiment with some parameters of random forest.\nWe will try different values for n_estimator (number of trees).</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\nparam_to_test = {'n_estimators': [100, 500, 1000, 2000]}\n\n# for max_features:\n# If \u201cauto\u201d, then max_features=sqrt(n_features).\n# If \u201csqrt\u201d, then max_features=sqrt(n_features) (same as \u201cauto\u201d).\n# If \u201clog2\u201d, then max_features=log2(n_features).\n# If None, then max_features=n_features. \n# param_to_test = {'n_estimators': [100, 500, 1000, 2000], \n#                 'max_features': ['auto', 'sqrt', 'log2', 'None']}\n\n# Use out-of-bag samples to estimate the generalization accuracy.\nrf_classifier2 = RandomForestClassifier(oob_score=True, random_state=0)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\n# Experiments with n-fold cross validation\nfrom sklearn.model_selection import GridSearchCV\n\n# 10-fold cross validation, and return training score\ngrid10 = GridSearchCV(rf_classifier2, param_grid = param_to_test, cv = 10, scoring ='accuracy', return_train_score = True)\n\n# Do the training\ngrid10.fit(X_train,y_train)\ngrid10.cv_results_\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\n# Find the best model and the score from the cross validation experiment\n\nprint('The best model:', grid10.best_estimator_)\nprint('The best score: ', grid10.best_score_)\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "source": ["\n<p>Let's do experiment to know the number of features to be used.</p>\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["\n# To use Stratified K-Folds cross-validator\n# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html \nfrom sklearn.model_selection import StratifiedKFold\n\n# To use recursive feature eliminatin with cross-validation (RCECV)\n# https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py\nfrom sklearn.feature_selection import RFECV\n\n# Use the best model from 10-fold cross validation experiment\n# Copy from the result above\nmodel = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=None,\n            oob_score=True, random_state=0, verbose=0, warm_start=False)\n\n# Create the RFE object and compute a cross-validated score.\nrfecv = RFECV(model, step=1, cv=10)\n# fit = rfecv.fit(X_train, y_train)\n\n# The \"accuracy\" scoring is proportional to the number of correct\n# classifications\nrfecv = RFECV(estimator=model, step=1, cv=StratifiedKFold(n_splits=10),\n              scoring='accuracy')\nrfecv.fit(X_train, y_train)\n\nprint(\"Optimal number of features : %d\" % rfecv.n_features_)\n\n# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()\n\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["\n \n\n"], "execution_count": null, "cell_type": "code"}], "metadata": {}}